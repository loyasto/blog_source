---
title: 爬虫（1）
date: 2018-06-28 19:20:48
tags:
	- 爬虫

---



这个是对这个系列教程的学习记录。

http://python.jobbole.com/89091/

基于Python3，我安装的是python3.7 的。



先用urllib来获得一个网页的内容。

新建get_html.py。

```
from urllib.request import urlopen

html = urlopen("http://jr.jd.com")
print(html.read())
html.close()
```

这个页面代码700行左右，不多，但是实现的效果很好。调用了不少其他的脚本。

urllib目录下，只有5个文件：

```
error.py
parse.py
request.py：这个最多，100K。
response.py
robotparser.py
```



用bs4来改进一下。

我们获取一下标题里的分类。

```
from urllib.request import urlopen
from bs4 import BeautifulSoup
#这个是京东金融的首页
html = urlopen("http://jr.jd.com")
bs_obj = BeautifulSoup(html.read(), 'html.parser')
text_list = bs_obj.find_all("a", "nav-item-primary")
for text in text_list:
    print(text.get_text())

html.close()
```

得到的内容：

```
首页
理财
众筹
保险
白条
股票
东家财富
企业金融
金融云
城市计算
```

现在我们来爬取网易云音乐上播放数大于500万的歌单。

歌单的地址是：

http://music.163.com/#/discover/playlist

这个网页的内容有2200行左右。
我们随便看一个歌单上的播放数量，然后在网页源码里搜索这个数字，奇怪，怎么找不到呢？

因为这个是一个动态的网页，内容是动态生成的。

是js代码生成的。

```
不对，不能直接看源代码的。要在chrome里按F12，打开调试界面，在搜索，可以搜索到这个数字。
```



我们要得到这个数字，有两种思路：

1、直接从js代码里采集。

2、用Python的第三方库运行js，直接采集你在浏览器里看到的页面。

我们选择第二种方式。

我们需要借助一个工具：Selenium。

安装一下：

```
pip install Selenium
```

Selenium自己不带浏览器，需要跟第三方浏览器结合一起使用。

我们需要另外一个工具。

PhantomJS。这个是一个“无头”（headless）浏览器。

它会把网站加载到内存并执行页面上的js代码。但是它不会向用户展示网页的图形界面。

把Selenium和PhantomJS结合在一起，就可以运行一个非常强大的网络爬虫了。

PhantomJS不是Python的第三方库，不能用pip安装。

我们到它的官网下载，然后放到Python3的scripts目录下。

http://phantomjs.org/download.html

下载后，解压得到一个phantomjs.exe文件。

放到C:\python37\Scripts目录下。

打开歌单首页，定位看到这些内容。

```
<a title="悲伤至极泪眼迷离|那些一听就哭的歌" href="/playlist?id=984506585" class="msk"></a>
```

```
<span class="nb">57593</span>
```

播放数，nb。

封面msk。有标题和url。

同样，可以找到“下一页”的url。

最后一页的url是“javascript:void(0)”。

我们改进后的内容如下：

```
from selenium import webdriver
import csv

url = 'http://music.163.com/#/discover/playlist/?order=hot&cat=%E5%85%A8%E9%83%A8&limit=35&offset=0'
driver = webdriver.PhantomJS()
csv_file = open('playlist.csv', 'w', newline='')
writer = csv.writer(csv_file)
writer.writerow(['标题', '播放数','链接'])

while url != 'javascript:void(0)':
    driver.get(url)
    driver.switch_to.frame("contentFrame")
    data = driver.find_element_by_id("m-pl-container").find_elements_by_tag_name("li")
    print(len(data))

    for i in range(len(data)):
        nb = data[i].find_element_by_class_name("nb").text
        if '万' in nb and int(nb.split("万")[0]) > 500:
            msk = data[i].find_element_by_css_selector("a.msk")
            writer.writerow([msk.get_attribute('title'), nb, msk.get_attribute('href')])

    url = driver.find_element_by_css_selector('a.zbtn.znxt').get_attribute('href')

csv_file.close()
```

我的运行会报错。

```
35
35
35
35
35
Traceback (most recent call last):
  File "D:/work/pycharm/spider/get_html.py", line 13, in <module>
    data = driver.find_element_by_id("m-pl-container").find_elements_by_tag_name("li")
  File "C:\python37\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 353, in find_element_by_id
    return self.find_element(by=By.ID, value=id_)
  File "C:\python37\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 957, in find_element
    'value': value})['value']
  File "C:\python37\lib\site-packages\selenium\webdriver\remote\webdriver.py", line 314, in execute
    self.error_handler.check_response(response)
  File "C:\python37\lib\site-packages\selenium\webdriver\remote\errorhandler.py", line 242, in check_response
    raise exception_class(message, screen, stacktrace)
selenium.common.exceptions.NoSuchElementException: Message: {"errorMessage":"Unable to find element with id 'm-pl-container'","request":{"headers":{"Accept":"application/json","Accept-Encoding":"identity","Connection":"close","Content-Length":"95","Content-Type":"application/json;charset=UTF-8","Host":"127.0.0.1:3921","User-Agent":"selenium/3.12.0 (python windows)"},"httpVersion":"1.1","method":"POST","post":"{\"using\": \"id\", \"value\": \"m-pl-container\", \"sessionId\": \"4bf9b320-7acd-11e8-9492-f3bbcd7ca96a\"}","url":"/element","urlParsed":{"anchor":"","query":"","file":"element","directory":"/","path":"/element","relative":"/element","port":"","host":"","password":"","user":"","userInfo":"","authority":"","protocol":"","source":"/element","queryKey":{},"chunks":["element"]},"urlOriginal":"/session/4bf9b320-7acd-11e8-9492-f3bbcd7ca96a/element"}}
Screenshot: available via screen
```

可能是后面的页面不符合规律了。

我改一下，下面可以跑出结果来。

```
from selenium import webdriver
import csv

url = 'http://music.163.com/#/discover/playlist/?order=hot&cat=%E5%85%A8%E9%83%A8&limit=35&offset=0'
driver = webdriver.PhantomJS()
csv_file = open('playlist.csv', 'w', newline='')
writer = csv.writer(csv_file)
writer.writerow(['标题', '播放数','链接'])

count = 0
while count < 4: #这里只看前面4页的。
    driver.get(url)
    driver.switch_to.frame("contentFrame")
    data = driver.find_element_by_id("m-pl-container").find_elements_by_tag_name("li")
    print(len(data))

    for i in range(len(data)):
        nb = data[i].find_element_by_class_name("nb").text
        if True:#'万' in nb and int(nb.split("万")[0]) > 500:
            msk = data[i].find_element_by_css_selector("a.msk")
            try: #这里忽略Unicode错误。
                writer.writerow([msk.get_attribute('title'), nb, msk.get_attribute('href')])
            except UnicodeError as u:
                continue
    url = driver.find_element_by_css_selector('a.zbtn.znxt').get_attribute('href')
    count = count + 1

csv_file.close()
```

