---
title: AI之神经网络初步了解
date: 2017-12-29 16:17:53
tags:
	- AI
	- 神经网络
typora-root-url: ..\
---



神经网络的最重要的用途是分类。

什么是分类？先看几个简单的例子。

1、垃圾邮件识别。

现在有一封垃圾邮件，把里面的所有词汇都提取出来，送进一个机器，机器要判断这个邮件是否是垃圾邮件。

2、疾病判断。

病人做了一堆的检查，把检查数据输入到一个机器里，机器要能够判断出病人的病情。

3、猫狗分类。

有一堆照片，把里面的猫和狗找出来。

这种能够对输入的东西进行分类的机器，就叫分类器。

分类器的输入是数值向量，叫做特征。

分类器的输出是数值。

分类器的目标是让分类的正确率尽可能高。

我们先收集一些样本，人为标记正确的分类结果，然后用这些标记好的数据训练分类器。

如下图所示，x代表了猫，o代表了狗。要把这两组特征向量分开的方法是啥？就是在中间画一条直线，左边是猫，右边是狗。这样分类就完成了。

一条直线把一个平面一分为二，一个平面把一个三维空间一分为二，一个n-1维超平面把一个n维空间一分为二。

这种分类器就叫神经元。

![神经网络-1](/images/神经网络-1.png "神经网络-1")

我们都知道平面的直线方程是ax+by+c = 0。把这个公司扩展到n维空间里。对应的超平面方程是：

h = a1x1 + a2x2 + ... + anxn + a0 = 0

神经元就是当h大于0的时候，输出1，小于0时，输出0的一个模型。

它的实质就是把特征空间一分为二。这个是1943年就提出来的。两个提出者的姓的首字母各取一个，叫做MP模型。

这个模型有点像人脑中的神经元：从多个感受器接受电信号x1，x2，...，xn进行处理，发出电信号。

这就是为什么这个模型叫做神经元。

上面这幅图，是我们开了上帝视角才知道一条直线可以分开两类。在实际训练神经元的时候，我们并不知道特征是怎样分布的。一种简单的学习方法是Hebb算法。实际上是一种试错的方法。

先随机选一个直线，然后把样报一个个拿过来判断，如果错了，就调整一下直线位置。如果对了，就暂时不动。

这样最后直接就到合理的位置了。

MP模型有几个显著的缺点：

1、它把直线的一侧变为0，另一侧变为1，导致不可微分，不利于数学分析。解决办法是用Sigmoid函数来替代。这个函数的形状是一个平头的S形状。这样神经网络就可以用梯度下降法来构造了，这就是著名的反向传播算法。

2、只能切分一次。对于下图的情况就没法一次分类成功了。

![神经网络-2](/images/神经网络-2.png "神经网络-2")

解决的办法是多层神经网络，底层神经元的输出是高层神经元的输入。我们可以先横着来一刀，再竖着来一刀。

每砍一刀，就是使用了一个神经元。

只要你砍的刀数足够多，无论什么奇怪的边界都可以表示出来。所以说，神经网络在理论上可以表示很复杂的空间分布。

在实际中，神经网络是否能够正常区分要看网络的初始值设置、样本容量和分布。

神经网络的神奇之处在于它的组件很简单，但是可以一层层级联。

2012年，多伦多大学的研究人员构造了一个超大型卷积神经网络，有9层，共65万个神经元。

网络的输入是图片，输出是1000个分类。

这个模型的训练需要海量的图片，它的分类准确率也完爆了之前的所有分类器。

第一层神经元主要负责识别颜色和简单纹理。

第二层的一些神经元可以识别更加细致的纹理，例如布纹、刻度等。

第三层的神经元负责感受黑夜里的黄色烛光、鸡蛋黄、高光。

第四层的神经元负责识别狗的脸、七星瓢虫和一堆原形物体的存在。

第五层的神经元可以识别出花、键盘、鸟等。

